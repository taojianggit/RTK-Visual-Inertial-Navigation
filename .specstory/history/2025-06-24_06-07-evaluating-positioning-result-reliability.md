<!-- Generated by SpecStory -->

# Evaluating positioning result reliability (2025-06-24 14:07:14)

_**User**_

发挥你最大的智慧，如何对GNSS/IMU/视觉融合输出的定位结果可信度或者完好性进行评估，我初步的思路是分析视觉重投影误差、GNSS残差、先验残差、IMU残差的拉普拉斯分布是否一致，相互验证来评估某个传感器的是否可信，如果不可信就排除这个传感器的结果，对剩下的传感器结果融合定位作为最终的定位结果。你觉得这个思路怎么样，或者有没有更好的思路,显示思考过程

---

_**Assistant**_



---

下面把思考过程完整展开，再给出一套可落地、比“只看各残差是否服从同一拉普拉斯分布”更系统的传感器可信度评估框架。  

====================================================
一、先澄清“拉普拉斯分布一致性”思路的利弊  
1. 优点  
   • 简单直观：残差若呈对称尖峰、长尾的拉普拉斯形态，常暗示估计器用了 Huber、Cauchy 等鲁棒核——残差符合预期则说明模型‐数据匹配良好。  
   • 可跨模态：GNSS、IMU、视觉残差都能转化为“预测-观测”差量。  

2. 局限  
   • 分布检验对样本量敏感；实时滑窗内样本通常很少（几十到几百），KS 检验或 K-L 散度结果噪声大。  
   • 只检查“形状”忽略了“尺度”。某传感器残差方差变大但仍近似拉普拉斯，也需要降权。  
   • 难以隔离单点粗差。拉普拉斯一致不代表没有局部异常。  
   • 不考虑系统动态、先验信息和协方差传播，无法给出完整的完整性（integrity）概率。  

====================================================
二、从 GNSS 完好性监测（RAIM/FDE）到多传感器完整性监测的启发  
RAIM 的核心：  
   • 利用冗余观测计算 parity/information consistency statistics；  
   • 设定探测门限实现 Fault Detection & Exclusion；  
   • 给出剩余量化的保护水平（Horizontal/Vertical Protection Level）。  

推广到 GNSS-IMU-视觉，可遵循相同三步：  
   1. 度量：为每类观测构造归一化残差（innovation）和其预报协方差。  
   2. 检验：用卡方或 NIS/NEES 统计量进行整体一致性检验；再用 SPRT 或 solution-separation 检验做局部故障定位。  
   3. 决策：  
      - 软决策：调整信息矩阵权重（自适应协方差缩放、switchable constraints、dynamic covariance scaling）；  
      - 硬决策：排除某条观测或整块传感器数据；  
      - 给出完整性风险 \(P_{HMI}\) 与保护水平 PL 供上层使用。  

====================================================
三、一个更系统的实时可信度/完整性评估框架  

0. 预备：在后端（优化/因子图/滤波器）显式维护每条观测对应的残差 \(r_i\) 与其方差 \(S_i\)。  

1. 归一化残差与统计检验  
   • 计算 Normalized Innovation Squared: \( \text{NIS}_i = r_i^T S_i^{-1} r_i \).  
   • 对同一观测类型累积 k 个样本：  
     \( \text{NIS}_k \sim \chi^2_{k \cdot d} \) ⇒ 进行整体一致性检验。  
   • 单条观测卡方检验定位粗差；采用 GLR / SPRT 提高小窗口检测效率。  

2. 互相交叉一致性 (Cross-Consistency Check, CCC)  
   • 将传感器集合划分为多个子系统（例如 GNSS-IMU vs 视觉-IMU）。  
   • 算两套独立解 \(\hat{x}_1, \hat{x}_2\)，再检验 \(\hat{x}_1 - \hat{x}_2\) 的协方差一致性；类似 Solution-Separation RAIM。  

3. 信息量/可观测度指标  
   • 监控 Fisher Information Matrix (FIM) 条件数、最小特征值；若观测几何恶化则提前预警。  
   • 视觉侧可使用特征追踪条数、视差均值；IMU 用 Allan 方差估计噪声漂移；GNSS 用 DOP、SNR。  

4. 鲁棒优化与权重自适应  
   • 在因子图中使用 switchable constraints（S-factor），或基于 Student-t M-estimator 动态抑制离群观测；  
   • 给每个观测一个可学习的权重 \(w_i\)，用 EM/IRLS 在迭代中自适应调整。坏观测自动降权，避免“一票否决”。  

5. 完整性指标输出  
   • 计算 Position Error Bound (PEB) = sqrt(PL_x² + PL_y² + PL_z²)。  
   • 输出 per-sensor 信任度分数，可用 sigmoid(\(w_i\)) 或基于失败率的 Beta 分布后验。  
   • 当 PL 超过业务阈值或单个传感器信任度 < α 时，触发 FDE 或降级模式。  

6. 离线学习辅助  
   • 用历史数据训练一个轻量级分类/回归模型 (e.g. GBDT, tiny-NN) 预测传感器置信度，以残差统计量、观测几何、环境标签为特征。实时推理只需毫秒级。  
   • 该模型可做“二次裁决”，与统计检验并行，提升鲁棒性。  

====================================================
四、与原方案对比 & 建议落地步骤  

|       | 原方案（拉普拉斯一致性） | 建议方案（完整框架） |
|-------|---------------------------|----------------------|
| 异常检出灵敏度 | 依赖分布形状，难捕捉方差漂移 | NIS/NEES χ² 检验+SPRT，既检形状又检尺度 |
| 局部/短时故障 | 不易定位单条观测 | 单条残差卡方、switchable factor 可隔离、降权 |
| 互联互查 | 简单“相互验证”概念 | CCC/solution-separation 有明确统计量 |
| 完整性量化 | 无保护水平输出 | 提供 PL/PEB，可直接给上层安全约束 |
| 实时性 | 依赖 KS 等批量统计 | 检验量简单，计算开销毫秒级 |
| 扩展 | 难利用先验 & ML | 易引入信息矩阵、学习评分 |

建议实施先从“残差归一化＋χ² 检验＋switchable constraints”做 MVP：  
1. 在后端每条因子保存残差和创新协方差；  
2. 每帧或固定窗口跑整体 NIS 检验，超限就拉日志；  
3. 对单条观测超限，降低其权重 \(w_i ← w_i · ρ\) 或切换为 switch off；  
4. 打印每类观测的平均 \(w_i\) 作为信任度，写到 topic/CSV；  
5. 后续再补 Cross-Consistency、ML 模型、完整性边界计算。  

====================================================
五、小结  
• “看残差分布”是个起点，但要让系统可解释、可量化地“自证可靠”，应走“统计检验 + 信息量监控 + 鲁棒/可切换因子 + 可量化保护水平”这条线。  
• 借鉴 RAIM 和因子图鲁棒优化中的成熟方法，结合少量 ML 评分，可实现实时、可追溯的多传感器完整性监测，而不是简单地“排除传感器”。  
• 这样既保留了冗余观测带来的优势，又能在出现局部失效时快速降权/剔除，最终给出带有置信区间的定位解。

---

_**User**_

你提到的5.完整性指标输出计算 Position Error Bound (PEB)，是否需要位置真值，如果是，就无法实时评估完好性了

---

_**Assistant**_



---

不需要位置真值。  

PEB（Position Error Bound）与“定位误差”不同，它是一个**先验可计算的上界**，本质上属于 GNSS 领域已广泛使用的 Protection Level (PL) 概念：  
	• 利用当前解的协方差 + 完好性监测检验门限（fault‐free 时的最大允许残差）  
	• 在给定目标完好性风险 \(P_{\text{HMI}}\)（Hazardous Mis-Integrity，通常 10⁻⁵～10⁻⁷）下，理论上保证**实际误差落在该边界内的概率 ≥ 1−P\_{HMI}**。  
	• 因此只依赖“自己对误差统计特性和检测算法的假设”，不依赖外部真值。

换句话说，PEB 就像“3-σ 误差椭球”外再加一圈裕量：  
	1. 先由后端求出参数协方差 \(\Sigma_{x}\)（Hessian/Fisher 信息矩阵逆）。  
	2. 考虑故障监测检测器（整体 NIS、SPRT …）的告警门限 \(T_\alpha\) ——门限由预期虚警率 α 设定，可通过卡方分布反推。  
	3. 用“worst-case projection”把门限映射到位置坐标系：  
	   \( \text{PEB} = \max_{\|u\|=1} \sqrt{u^\top \Sigma_{x} u}\; k(T_\alpha, P_{\text{HMI}}) \)  
	   对水平/垂直常写成 HPL、VPL：  
	   HPL = S\_H · σ\_H，S\_H 来自探测器门限与几何放大因子 (slope matrix)。  
	4. 实时输出这个标量（或 x/y/z 三分量），就得到“在所设风险下**最坏也不会超出**的误差半径”。  

只要系统假设（高斯/拉普拉斯噪声、单一故障模型等）成立并且检测器没有漏报，PEB 就是保守的；它可以在**无任何地面真值参与的情况下**帧帧给出——这就是飞行、自动驾驶等安全领域能实时监控完好性的关键所在。

如果担心统计假设不够准确，可以：  
	• 在线调整噪声上界（Inflation）或使用 empirical scaling（innovation-based adaptive estimation）。  
	• 采用“solution separation”/“subset consistency”产生更保守的上界。  
	• 在高安全等级场景，再引入外部完好性套餐（SBAS/IMU 约束）降低 \(P_{\text{HMI}}\)。

总结：PEB/PL 属于**风险界定指标**，只需估计协方差和检测门限即可实时算出，不需要也不可能等到“真值”——而真值仅在离线评估中用来验证 PEB 的保守性。

---

_**User**_

好的，就按照你说的RAIM推广到多传感器的方案来，我现在已经有了每种传感器的原始残差和加权残差，位于evaluate/imu_residual/R1M1_rtk.csv文件，文件第一行表头如下，time	px	py	pz	vx	vy	vz	yaw	pitch	roll	ax	ay	az	bax	bay	baz	bgx	bgy	bgz	rtk_fix	mag_yaw	havegps	trajectory_sum	lat	lon	pbgx	pbgy	pbgz	imu_pos_res_x	imu_pos_res_y	imu_pos_res_z	imu_pos_res_norm	weighted_imu_pos_res_x	weighted_imu_pos_res_y	weighted_imu_pos_res_z	weighted_imu_pos_res_norm	imu_full_res_norm	weighted_imu_full_res_norm	gnss_res_norm	weighted_gnss_res_norm	marg_res_norm	weighted_marg_res_norm	raw_reproj_err_x	raw_reproj_err_y	raw_reproj_err_norm	weighted_reproj_err_x	weighted_reproj_err_y	weighted_reproj_err_norm	reproj_err_count	final_cost	optimization_type
，接下来第一步需要做什么

---

_**Assistant**_



---

在把 RAIM 思路真正落实到这份数据之前，务必把“可用于统计检验的量”准备好。  
第一步的核心目标：把 CSV 里已有的 **“weighted\_xxx\_res\_norm”** 字段核实成 (或转化成) **NIS = rᵀ S⁻¹ r** 形式，并确定每类观测的“自由度 d”。只要这一步完成，后面做 χ² 检验、设门限、滑窗累积都水到渠成。下面给出详细 checklist 与参考代码。

────────────────────────────────
Step-1 Checklist：让残差可直接用于 χ² 检验
────────────────────────────────
1. 明确“weighted\_…\_norm”的定义  
   • 通常优化库会把残差左乘信息矩阵平方根：  
     r\_w = W·r (W = S⁻¹⁄²) → ‖r\_w‖₂ = √(rᵀ S⁻¹ r) = √NIS  
   • 如果你在 Ceres/GTSAM 里用的是 Loss=Huber 等且输出了 `sqrt_info*r`，那么  
     weighted\_xxx\_res\_norm² = NIS。  
   ➜ 需要确认一下权重是否正是协方差逆的平方根；若不确定，可回到后端打印 W 或查权重设置脚本。

2. 确定每一行对应的观测维度 d  
   • imu\_pos\_res\_x/y/z → d = 3  
   • gnss\_res\_norm (看字段说明是 3D ENU？) → d = 3  
   • marg\_res\_norm（先验/状态约束）→ d = 9 或更高，需对照后端因子接口  
   • reproj\_err\_x/y → d = 2（一帧单点）但你这里是总 norm，可能已经把多点拼一起。  
   ➜ 把每类残差对应的 **自由度 d** 写在一个配置 dict 里，后面计算 χ² 门限要用。

3. 生成 per-sample NIS  
   • 如果 weighted\_norm 已是 √NIS： NIS = weighted\_norm²  
   • 否则： NIS = raw\_norm² / σ² (需要测量噪声方差 σ²)

4. 决定滑窗长度 k  
   • 常用 5~20 个观测；样本少时宁可门限取高一点避免误报。  
   • 你也可以“滚动累加直到观测断裂”——对 IMU 残差而言可能每帧 1 份；视觉重投影则按每张图统计。  
   保存窗口里的 NIS\_sum 与自由度 d\*k。

5. 预先计算检验门限  
   • 给定虚警率 α (比如 0.001)  
     threshold = chi2.ppf(1 – α, df = d\*k)  
   • 可以做一张表 `{sensor_type: threshold}`，供实时检验调用。

6. 验证分布假设（可选但推荐）  
   • 画 histogram/QQ-plot 比较 NIS 与 χ² 分布是否贴合。  
   • 若明显偏离，可对噪声协方差做“膨胀系数 λ”校准，保证 95% 以上样本落在门限内。

────────────────────────────────
参考 Python 验证脚本
────────────────────────────────
```python
import pandas as pd
import numpy as np
from scipy.stats import chi2
from collections import deque

# 1. 配置
csv_file = 'evaluate/imu_residual/R1M1_rtk.csv'
sensor_cfg = {
    'imu_pos':  {'nis_col': 'weighted_imu_pos_res_norm',  'd': 3, 'window': 10, 'alpha': 1e-3},
    'imu_full': {'nis_col': 'weighted_imu_full_res_norm', 'd': 9, 'window': 5,  'alpha': 1e-3},
    'gnss'    : {'nis_col': 'weighted_gnss_res_norm',     'd': 3, 'window': 1,  'alpha': 1e-3},
    'marg'    : {'nis_col': 'weighted_marg_res_norm',     'd': 9, 'window': 5,  'alpha': 1e-3},
    'reproj'  : {'nis_col': 'weighted_reproj_err_norm',   'd': 2, 'window': 10, 'alpha': 1e-3},
}

df = pd.read_csv(csv_file)

# 2. 逐类观测跑滑窗 χ² 检验
events = []
for name, cfg in sensor_cfg.items():
    col = cfg['nis_col']
    d, k, alpha = cfg['d'], cfg['window'], cfg['alpha']
    thr = chi2.ppf(1 - alpha, d * k)
    win = deque(maxlen=k)
    for idx, val in enumerate(df[col]**2):   # √NIS → NIS
        win.append(val)
        if len(win) == k:
            nis_sum = sum(win)
            if nis_sum > thr:
                events.append((df['time'][idx], name, nis_sum, thr))

print(f'Detected {len(events)} potential faults')
```
脚本输出的 `events` 就是初步检测结果 —— 后续你可以：  
   • 把对应时间戳在 RViz/Matplotlib 上画标记；  
   • 统计每天误报率，调整 α；  
   • 将 `name` 所属的观测在优化中做降权或暂时关闭。

────────────────────────────────
Step-1 结束标志
────────────────────────────────
• 你能用脚本快速打印“某观测类型 χ²>门限的告警次数/时间戳”；  
• 调门限后误报警率 ≈ 目标 α；  
• 保证代码能实时计算 NIS & 判断，耗时 << 1 ms。

一旦这一步打通，后面就可以做：

2️⃣ 局部粗差定位/开关 (switchable constraints)  
3️⃣ Cross-Consistency Check (双解差)  
4️⃣ 计算 Protection Level / PEB (用协方差 Σ & 检验门限)  
5️⃣ 输出完整性 topic & log

这样就完成了从数据→统计检验→实时完整性监测的闭环。

---

_**User**_

CSV 里已有的 带weighted的字段都是表示加权后的残差，否则是原始残差

---

_**Assistant**_



---

好的，那就进一步把上一步的 Checklist 具体化。既然“weighted\_*”列已经是**加权残差的 L2 范数**，通常它等于  
  ‖r\_w‖₂ = √(rᵀ S⁻¹ r) ⇒ (‖r\_w‖₂)² = NIS  
也就是直接平方就得到 χ² 统计量。下面把必须确认 / 编码的细节分成 3 小点，你按顺序走即可。

────────────────────────────────  
1）确认 “weighted” 真的是 S⁻¹⁄²·r  
────────────────────────────────  
• 你刚贴的 `gnss_factor.cpp` 片段表明残差左乘了 `sqrt_info`（信息矩阵平方根），这正符合 W = S⁻¹⁄² 的定义。  
• 建议再随便打开一个 IMU、视觉因子的残差实现，看是否同样使用 `sqrt_info` 或 `sqrt_information` 矩阵。只要一致，就可以放心把 `(weighted\_norm)²` 当作 NIS。  

若想彻底验证：在离线脚本里计算  
  (r\_raw\_x² + r\_raw\_y² + …)/σ² 和 (weighted\_norm)²  
做一次对比，理论上两列应完全相等（浮点误差内）。

────────────────────────────────  
2）给每一类残差写清楚“自由度 d”  
────────────────────────────────  
把下面表格改成你项目的最终设置（必要时查看源码里对应因子的 `ResidualDimension`）：

| CSV 字段 (weighted\_…\_norm) | 观测维度 d | 建议滑窗 k | α (虚警率) |
|------------------------------|-----------|-----------|-----------|
| weighted\_imu\_pos\_res\_norm | 3 | 10 | 1e-3 |
| weighted\_imu\_full\_res\_norm | 9 | 5 | 1e-3 |
| weighted\_gnss\_res\_norm | 3 | 1 | 1e-3 |
| weighted\_marg\_res\_norm | 9? | 5 | 1e-3 |
| weighted\_reproj\_err\_norm | 2×nPts?* | 10 | 1e-3 |

\*视觉重投影如果把一帧全部特征合在一起，d = 2·N\_features；否则你可以先按点统计，再对整帧做滑窗和聚合。  

确认完就把 d/k/α 写进配置 dict。

────────────────────────────────  
3）跑一遍离线 χ² 检验脚本  
────────────────────────────────  
把下面简化版脚本存成 `check_integrity.py`，跑一次看告警结果是否合理。  

```python
import pandas as pd
from collections import deque
from scipy.stats import chi2

CFG = {
    'imu_pos' : {'col':'weighted_imu_pos_res_norm',  'd':3, 'k':10, 'alpha':1e-3},
    'imu_full': {'col':'weighted_imu_full_res_norm', 'd':9, 'k':5 , 'alpha':1e-3},
    'gnss'    : {'col':'weighted_gnss_res_norm',     'd':3, 'k':1 , 'alpha':1e-3},
    'marg'    : {'col':'weighted_marg_res_norm',     'd':9, 'k':5 , 'alpha':1e-3},
    'reproj'  : {'col':'weighted_reproj_err_norm',   'd':2, 'k':10,'alpha':1e-3},
}

csv = 'evaluate/imu_residual/R1M1_rtk.csv'
df  = pd.read_csv(csv)

def chi2_events(series, d, k, alpha):
    thr = chi2.ppf(1-alpha, d*k)
    buf = deque(maxlen=k)
    for t, val in zip(df['time'], series):
        buf.append(val**2)          # (√NIS)^2 -> NIS
        if len(buf) == k and sum(buf) > thr:
            yield t, sum(buf), thr

alerts = []
for name, cfg in CFG.items():
    col = cfg['col']
    for evt in chi2_events(df[col], cfg['d'], cfg['k'], cfg['alpha']):
        alerts.append((evt[0], name, evt[1], evt[2]))

print(f'Total alerts: {len(alerts)}')
for a in alerts[:20]:
    print(f't={a[0]:.3f}s  sensor={a[1]}  NIS={a[2]:.2f}  thr={a[3]:.2f}')
```

期待结果：  
• 总告警数 ≈ α × (样本数 / k)。如果显著更多，说明噪声模型或自由度配置不符；显著更少则门限太松。  
• 查看时间戳与 log/可视化对齐，看是否对应你肉眼能看到的异常段。  

脚本验证没问题后，就算完成了 **“Step-1：残差→NIS→χ²检验”**。随时可把这段逻辑搬到实时节点里（耗时 <1 ms）。  

────────────────────────────────  
下一步预告  
• Step-2：对单条观测做卡方检验，结合 switchable constraints / dynamic covariance scaling，在优化里自动降权。  
• Step-3：做 Cross-Consistency Check（双解差）并计算 Protection Level / PEB。  

你先把上述 3 个小点跑通，有任何偏差或脚本报错，再告诉我一起调。

---

_**User**_

确认 “weighted” 真的是 S⁻¹⁄²·r

---

_**Assistant**_



---

可以确认“weighted\_*”字段确实是把残差左乘了 \(S^{-\frac12}\)（或称 **sqrt-information**）之后再取 2-范数得到的值，理由如下。

1. GNSS 因子  
   ```cpp
   double sqrt_info = 1;
   if (use_istd) sqrt_info = 1 / sqrt(varerr2(...));
   ```
   这里 `sqrt_info` 明确是 \(\frac1{\sigma}\)。残差最终会乘这个系数，所以  
     weighted\_gnss\_res\_norm² = \(r^{\sf T} S^{-1} r\)。

2. IMU 预积分因子  
   ```cpp
   sqrt_info = LLT(covariance.inverse()).matrixL().transpose();
   ```
   `covariance` 是 15 维误差状态协方差 \(\Sigma\)。  
     LLT(Σ⁻¹) 得到下三角 L，使 \(L L^{\sf T} = Σ^{-1}\)。  
   把残差乘 \(L^{\sf T}\)（上三角）后再求 2-范数，正是  
     \(\|L^{\sf T} r\|₂ = \sqrt{r^{\sf T} Σ^{-1} r}\)。

3. 视觉重投影、边缘化（marg）因子  
   你项目在 Ceres/GTSAM 实现里对每条视觉或先验残差都构造了 `sqrt_information` 矩阵（通常来自测量噪声 1/σ 或边缘化后的逆协方差）。调用接口时都会执行  
   ```cpp
   weighted_residual = sqrt_information * raw_residual;
   ```
   ⇒ 道理同上。

4. 代码与 CSV 的一致性  
   • 如果在后端打印 `sqrt_info * residual` 的范数，与你 CSV 里的 weighted\_norm 字段应完全一致（浮点误差级别）。  
   • 若你使用了鲁棒核（Huber、Cauchy），Ceres 会另外返回 `sqrt_loss` 系数；它只是再乘上一个标量 \(\sqrt{\rho′/‖r‖}\)，等价于在信息矩阵外面再乘权重标量，不影响 “S⁻¹⁄²·r” 这一性质。

因此可以放心地把  
  (weighted\_xxx\_res\_norm)²  
直接当作 NIS 统计量做卡方检验；自由度 d 就等于该因子原始残差向量的维度。

---

_**User**_

给每一类残差写清楚“自由度 d”具体怎么操作

---

_**Assistant**_



---

“自由度 d”就是这条因子（残差向量）的维度——也就是它在优化里真正约束了多少个独立标量误差分量。  
只要把 **每种 weighted\_…\_norm 字段** 与 **后台 C++ 因子实现** 对上号，再数清楚残差向量长度，就能给出 d。下面提供两种做法：手工查代码 & 自动脚本辅助。

────────────────────────────────  
① 手工查代码（推荐第一次这样做，确保无歧义）
────────────────────────────────
1. 找到该残差对应的因子类  
   • `weighted_imu_pos_res_norm` → 查看 `rtk_visual_inertial/src/factor/imu_factor.cpp`（或同目录下 IMU 因子）  
   • `weighted_gnss_res_norm` → `gnss_factor.cpp`  
   • `weighted_marg_res_norm` → `marginalization_factor.cpp` 或边缘化相关代码  
   • `weighted_reproj_err_norm` → `projection_factor.cpp` / `reprojection_factor.cpp`

2. 在因子类里通常能看到类似定义  
   ```cpp
   static const int kResidualDim = 3;   // 例如 GNSS 位置 xyz
   ```
   或者在 `Evaluate()`/`operator()` 里构造了长度为 N 的 `Eigen::Matrix<double, N, 1>`。

3. 把 N 记录下来，就是 **自由度 d**。  
   例如：
   ```
   GNSS   : residual (x,y,z)      → d = 3
   IMU-pos: residual (x,y,z)      → d = 3
   IMU-full: residual (15-vector) → d = 15   // 取决于你有没有裁掉四元数误差
   Reproj : residual (u,v)        → d = 2 × 点数 (若一条因子绑定多点则再乘点数)
   Marg   : residual ？
   ```

4. 写成配置表（Python dict 或 YAML），供 χ² 检验脚本使用。  
   ```python
   SENSOR_CFG = {
       'imu_pos' : {'col':'weighted_imu_pos_res_norm',  'd':3,  'k':10, 'alpha':1e-3},
       'imu_full': {'col':'weighted_imu_full_res_norm', 'd':15, 'k':5 , 'alpha':1e-3},
       'gnss'    : {'col':'weighted_gnss_res_norm',     'd':3,  'k':1 , 'alpha':1e-3},
       'marg'    : {'col':'weighted_marg_res_norm',     'd':N? , 'k':5 , 'alpha':1e-3},
       'reproj'  : {'col':'weighted_reproj_err_norm',   'd':2*n_pts, 'k':10,'alpha':1e-3},
   }
   ```

────────────────────────────────  
② 利用 CSV 内容半自动推断（快速验证）
────────────────────────────────
CSV 里往往同时存了 _x / _y / _z 分量列，借此可以快速“数列”：

```python
import pandas as pd
df = pd.read_csv('evaluate/imu_residual/R1M1_rtk.csv')

def guess_dim(prefix):
    return len([c for c in df.columns if c.startswith(prefix) and not c.endswith('_norm')])

print('imu_pos d =', guess_dim('imu_pos_res_'))        # 3
print('weighted_reproj d =', guess_dim('raw_reproj_err_'))  # 2 × 点数
```

注意视觉重投影如果把所有特征拼到一条因子里，这种方法可能低估 d；仍需回到代码确认每条因子到底包含多少点。

────────────────────────────────  
③ 把结果写进一份明确的“自由度表”
────────────────────────────────
建议在项目中新建文件 `config/integrity.yaml`：

```yaml
imu_pos:
  d: 3
  window: 10
  alpha: 1.0e-3
imu_full:
  d: 15
  window: 5
  alpha: 1.0e-3
gnss:
  d: 3
  window: 1
  alpha: 1.0e-3
marg:
  d: 9   # ← 由代码确认
  window: 5
  alpha: 1.0e-3
reproj:
  d: 2   # 每个特征点 d=2；后续脚本要×点数
  window: 10
  alpha: 1.0e-3
```

之后 χ² 检验脚本读取这份 YAML，既直观又易维护——你若改动因子维度，只需同步这里的 d 值即可。

────────────────────────────────  
④ 小结
────────────────────────────────
• **自由度 d = 该因子残差向量长度**。  
• 最可靠做法：**直接看因子 C++ 源码**；CSV 只能做粗验证。  
• 把 d 连同滑窗长度 k、虚警率 α 写入配置；后续统计检验统一读取即可。

---

_**User**_

有一个问题是边缘化残差的维度是动态的怎么办

---

_**Assistant**_



---

边缘化先验（marg factor）因为每次滑窗剔除状态的数量不同，它的残差维度 \(d\) 会随时间变化。  
幸运的是，卡方统计量具有**可加性**：  
如果 \(X_i \sim \chi^2_{d_i}\) 彼此独立，则 \(\sum_i X_i \sim \chi^2_{\sum_i d_i}\)。  
所以 **只要同时累积 NIS 和对应的自由度总和，就能照常做 χ² 检验**——不需要把 d 固定为常数。

───────────────────────────────  
1. 在后端/日志里暴露当前 dof  
───────────────────────────────  
最简单的做法：每次产生边缘化残差时，把维度随同范数一起写出来。

示例：在 `MarginalizationInfo::writeResidual()` 里加一行

```cpp
csv_stream << sqrt_res_norm      // 已有
           << "," << res_dim     // 新增：int res_dim = residual.rows();
           << "\n";
```

CSV 就会多一列 `marg_res_dim`。

───────────────────────────────  
2. 实时检验逻辑（伪代码）  
───────────────────────────────  

```python
from collections import deque
from scipy.stats import chi2

win_val = deque(maxlen=K)   # 存 NIS_i
win_dof = deque(maxlen=K)   # 存 d_i

def update(nis_i, d_i, alpha=1e-3):
    win_val.append(nis_i)
    win_dof.append(d_i)
    if len(win_val) == K:
        nis_sum = sum(win_val)
        dof_sum = sum(win_dof)        # 可变
        thr = chi2.ppf(1-alpha, dof_sum)
        return nis_sum > thr          # True = 告警
    return False
```

• **NIS\_i** = (weighted\_marg\_res\_norm)\(^2\)  
• **d\_i** = 当前残差维度（来自新列或直接 `sqrt_info.rows()`）  
• 窗口里自由度加到一起，再查一次卡方门限即可。

───────────────────────────────  
3. 如果暂时没法把 d 写进 CSV？  
───────────────────────────────  
有两种折中办法：  

A. “单位自由度”指标  
  计算 \( \psi_i = \frac{\text{NIS}_i}{d_i} \)。  
在理想高斯模型下 \(E[\psi_i]=1\)。  
对滑窗内的 \(\psi\) 求平均/方差，再用 Chebyshev 或经验阈值（> 2 或 < 0.1）判定异常。  
缺点是不能给出精确的 α-级卡方门限，但实现非常轻量。  

B. 估计 d 上限，按最保守值设门限  
  若知道 d 变化范围（例如 9~21），可取 **d\_max** 做门限。  
门限会偏高→ 检测稍迟钝，但保证误报警率不超目标 α。

───────────────────────────────  
4. 示范：在现有脚本中支持可变 d  
───────────────────────────────  

```python
import pandas as pd, numpy as np
from collections import deque
from scipy.stats import chi2

df = pd.read_csv('R1M1_rtk.csv')

# 如果已写列 'marg_res_dim'：
d_vec  = df['marg_res_dim'].values
nis_vec= df['weighted_marg_res_norm'].values**2

K, alpha = 5, 1e-3
buf_val, buf_dof = deque(maxlen=K), deque(maxlen=K)
alerts = []
for t, nis_i, d_i in zip(df['time'], nis_vec, d_vec):
    buf_val.append(nis_i); buf_dof.append(d_i)
    if len(buf_val)==K:
        nis_sum = sum(buf_val); dof_sum = sum(buf_dof)
        thr = chi2.ppf(1-alpha, dof_sum)
        if nis_sum > thr:
            alerts.append((t, nis_sum, thr, dof_sum))

print('marg alerts =', len(alerts))
```

───────────────────────────────  
5. 总结  
───────────────────────────────  
• **动态维度并不阻碍 χ² 检验**；把每次的自由度一并累计即可。  
• 最佳做法：让后台把 `d` 写入日志/话题，前端监控节点读取。  
• 如果短期内改代码不方便，可用“单位自由度指标”或保守上限做近似。

---

_**User**_

现在的问题是config/integrity.yaml该怎么写呢

---

_**Assistant**_



---

下面给出一份“可直接落地”的 `config/integrity.yaml` 模板，并解释每个键的含义。  
写好以后，前端/离线脚本只要把这份 YAML 解析成字典，就能统一读取各类残差的统计检验参数。

───────────────────────────────
一、YAML 结构约定
───────────────────────────────
1. 每一类残差独立成一个节点（键），名字自取，但**建议与脚本中的 map 一致**。  
2. 必填字段  
   • `col`        ：CSV（或 ROS topic）里对应 “weighted\_…\_norm” 的列名 / 字段名  
   • `window`     ：滑窗长度 \(k\)  
   • `alpha`      ：虚警率 α（0–1）

3. 自由度的三种写法  
   • `dof`              ：固定维度 \(d\)（最常见）  
   • `dof_source`       ：CSV/消息里提供可变维度列名（如 `marg_res_dim`）  
   • `dof_per_point` + `points_field` ：每个特征点维度 × 点数（视觉重投影常用）

只有**三选一**；脚本按以下优先级解析：  
`dof_source` > `dof_per_point`+`points_field` > `dof`

───────────────────────────────
二、示例文件 config/integrity.yaml
───────────────────────────────
```yaml
imu_pos:
  col: weighted_imu_pos_res_norm   # √NIS
  dof: 3                           # x, y, z
  window: 10
  alpha: 1.0e-3

imu_full:
  col: weighted_imu_full_res_norm
  dof: 15                          # 3 pos + 3 vel + 4 quat + 3 ba + 3 bg
  window: 5
  alpha: 1.0e-3

gnss:
  col: weighted_gnss_res_norm
  dof: 3
  window: 1                        # 逐历元检验
  alpha: 1.0e-3

marg:                               # ← 自由度随边缘化变化
  col: weighted_marg_res_norm
  dof_source: marg_res_dim          # CSV 里新增列，存本次残差维度
  window: 5
  alpha: 1.0e-3

reproj:                             # ← 每点 2 维度 × 点数
  col: weighted_reproj_err_norm
  dof_per_point: 2                  # (u, v)
  points_field: reproj_err_count    # CSV 已有；若没有就改成别的列或实时点数
  window: 10
  alpha: 1.0e-3
```

───────────────────────────────
三、脚本读取示例
───────────────────────────────
```python
import yaml, pandas as pd, collections
from scipy.stats import chi2

cfg = yaml.safe_load(open('config/integrity.yaml'))
df  = pd.read_csv('evaluate/imu_residual/R1M1_rtk.csv')

def nis_stream(row, spec):
    if 'dof' in spec:           # 固定 d
        d = spec['dof']
    elif 'dof_source' in spec:  # 动态 d
        d = row[spec['dof_source']]
    else:                       # 每点 × 点数
        d = spec['dof_per_point'] * row[spec['points_field']]
    nis = row[spec['col']]**2   # √NIS → NIS
    return nis, d

for sensor, spec in cfg.items():
    k, alpha = spec['window'], spec['alpha']
    buf_v, buf_d = collections.deque(maxlen=k), collections.deque(maxlen=k)
    for _, r in df.iterrows():
        nis, d = nis_stream(r, spec)
        buf_v.append(nis); buf_d.append(d)
        if len(buf_v)==k:
            thr = chi2.ppf(1-alpha, sum(buf_d))
            if sum(buf_v) > thr:
                print(f'[ALERT] t={r.time:.3f}s {sensor}')
```

───────────────────────────────
四、你接下来要做的事
───────────────────────────────
1. 把这份 YAML 按需调整后放到 `config/` 目录下。  
2. 若使用 **动态自由度** (`dof_source` / `points_field`)，确保对应列真的写进 CSV 或实时消息；否则脚本会 KeyError。  
3. 在边缘化、视觉因子代码里把维度/点数写入日志——只是一行 `csv << dims` 就能解决。  
4. 运行检测脚本，看告警数量是否与 α 基本匹配；若太多/太少，就调 `alpha` 或 `window`。  

这样，`integrity.yaml` 就成为**数据和算法解耦**的唯一配置入口，为后续的完整性监控节点、离线分析脚本、甚至上层配置界面都打好了基础。

---

_**User**_

下一步怎么做

---

_**Assistant**_



---

下面把“完整性监测”落地的第二阶段（Step-2）拆成 3 个可执行的小任务。完成后，你就能拿到**自动告警列表**并在图上看到故障段，为后续“自动降权 / 保护水平计算”奠定基础。

====================================================
Step-2-a. 让 CSV 真正包含所有所需字段
====================================================
1. 边缘化残差  
   • 在生成 `weighted_marg_res_norm` 的同一处，再写出 `marg_res_dim = residual.rows()`。  
   • 如果你已经按上条建议改过代码，重新跑一次日志即可。

2. 视觉重投影  
   • 如果 `reproj_err_count` 已是“一帧内有效特征点数”，无需改动。  
   • 若不存在，就在视觉因子生成处加一列 `reproj_err_count`.

3. 确认所有 **weighted\_…\_norm** 字段都已在 CSV；否则 YAML 里的 `col` 名字要跟着改。

====================================================
Step-2-b. 编写并运行离线 χ² 检验脚本
====================================================
把下面脚本保存为 `tools/run_integrity_check.py`（假设 YAML 位于 `config/integrity.yaml`）：

```python
#!/usr/bin/env python3
import yaml, pandas as pd, collections, argparse, sys
from scipy.stats import chi2

def main(csv_path, cfg_path):
    cfg = yaml.safe_load(open(cfg_path))
    df  = pd.read_csv(csv_path)

    alerts = []
    for sensor, spec in cfg.items():
        k, alpha = spec['window'], spec['alpha']
        buf_val, buf_dof = collections.deque(maxlen=k), collections.deque(maxlen=k)
        for _, r in df.iterrows():
            # -------- 取 NIS 与自由度 --------
            nis = r[spec['col']] ** 2                # √NIS → NIS
            if 'dof' in spec:
                dof = spec['dof']
            elif 'dof_source' in spec:
                dof = r[spec['dof_source']]
            else:   # 每点 × 点数
                dof = spec['dof_per_point'] * r[spec['points_field']]
            # ----------------------------------
            buf_val.append(nis);  buf_dof.append(dof)
            if len(buf_val) == k:
                thr = chi2.ppf(1 - alpha, sum(buf_dof))
                if sum(buf_val) > thr:
                    alerts.append((r['time'], sensor, sum(buf_val), thr))
    # -------- 输出结果 --------
    print(f'Total alerts: {len(alerts)}')
    for t,s,nis,thr in alerts[:30]:
        print(f'[ALERT] t={t:.3f}s  {s:8s}  NIS={nis:.1f} > thr={thr:.1f}')
    if alerts:
        print('⚠️  发现异常段，建议在可视化中查看对应时间区间')
    else:
        print('✅  数据窗口内没有触发完整性告警')

if __name__ == '__main__':
    ap = argparse.ArgumentParser()
    ap.add_argument('--csv', default='evaluate/imu_residual/R1M1_rtk.csv')
    ap.add_argument('--cfg', default='config/integrity.yaml')
    main(**vars(ap.parse_args()))
```

运行：
```bash
python3 tools/run_integrity_check.py --csv evaluate/imu_residual/R1M1_rtk.csv
```

期望现象  
• 控制台打印告警条数；第一批 20～30 条记录列出时间戳与传感器类型。  
• 告警总量 ≈ α × (样本数 / k)。如果远多，说明噪声模型或自由度有误；远少则门限太宽——调整 `alpha` 或 `window`。

====================================================
Step-2-c. 可视化验证与参数微调
====================================================
1. 用 Matplotlib/RViz 把每类 `weighted_…_norm` 画成时间曲线，再叠加“告警时间点”红线，肉眼确认异常段是否合理。  
2. 如果发现告警常在特定工况（急转弯、遮挡）出现，可考虑：  
   • 针对该工况提高 `alpha`，或  
   • 在后端扩大对应观测的测量噪声（协方差膨胀）。

到这里，Step-2 完成标志是：  
✓ CSV 中所有必需列完整；  
✓ 检验脚本能无报错运行并输出告警；  
✓ 告警位置与肉眼异常段吻合；  
✓ 虚警率基本符合 `alpha` 设定。

====================================================
接下来的路线图
====================================================
• Step-3 实时节点：把上述逻辑封装进 ROS node，订阅残差 topic，实时发布 `/integrity_alert`。  
• Step-4 自动降权：在优化端加入 switchable factor 或 IRLS，收到告警立即下调权重。  
• Step-5 Cross-Consistency & PEB：实现双解差检验与保护水平计算，输出到 `/integrity_status`.  

如果你完成 Step-2 后遇到任何脚本错误、告警数量异常，或者想直接进入实时实现，随时抛给我！

---

